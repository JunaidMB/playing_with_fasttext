{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!conda install gensim --y\n",
    "!conda install nltk --y\n",
    "!conda install spacy --y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/junaidbutt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/junaidbutt/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/junaidbutt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/junaidbutt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "import gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import models\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from numpy import asarray\n",
    "import unicodedata\n",
    "from botocore.client import Config\n",
    "import json\n",
    "from pytz import timezone\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fastText'...\n",
      "remote: Enumerating objects: 3826, done.\u001b[K\n",
      "remote: Total 3826 (delta 0), reused 0 (delta 0), pack-reused 3826\u001b[K\n",
      "Receiving objects: 100% (3826/3826), 8.20 MiB | 9.02 MiB/s, done.\n",
      "Resolving deltas: 100% (2409/2409), done.\n"
     ]
    }
   ],
   "source": [
    "# Install fasttext\n",
    "!git clone https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 1.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /Users/junaidbutt/anaconda3/envs/python_learning/lib/python3.7/site-packages (from fasttext) (2.5.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/junaidbutt/anaconda3/envs/python_learning/lib/python3.7/site-packages (from fasttext) (45.1.0.post20200127)\n",
      "Requirement already satisfied: numpy in /Users/junaidbutt/anaconda3/envs/python_learning/lib/python3.7/site-packages (from fasttext) (1.18.1)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-macosx_10_15_x86_64.whl size=325162 sha256=c4faf6806184addbc59ec1610b1dcc5f8ecdf1e1a5c6706830bc1ea2e2f38317\n",
      "  Stored in directory: /Users/junaidbutt/Library/Caches/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fastttext\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Files\n",
    "## Scandal in Bohemia sentences\n",
    "scandal_in_bohemia_sentences = open(\"scandal_in_bohemia_sentences.txt\", \"r\")\n",
    "scandal_in_bohemia_sentences = scandal_in_bohemia_sentences.readlines()\n",
    "\n",
    "scandal_in_bohemia_sentences_no_stopwords = open(\"scandal_in_bohemia_sentences_no_stopwords.txt\", \"r\")\n",
    "scandal_in_bohemia_sentences_no_stopwords = scandal_in_bohemia_sentences_no_stopwords.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "## NLP Functions\n",
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def preprocess(text, remove_accents=False, lower = True, remove_less_than=0, remove_more_than=100, remove_punct=True, \n",
    "               remove_alpha=False, remove_stopwords=True, add_custom_stopwords = [], lemma=False, stem=False, remove_url=True):\n",
    "    '''Tokenises and preprocesses text.\n",
    "    Parameters: \n",
    "    text (string): a string of text\n",
    "    remove_accents (boolean): removes accents\n",
    "    lower (boolean): lowercases text\n",
    "    remove_less_than (int): removes words less than X letters\n",
    "    remove_more_than (int): removes words more than X letters\n",
    "    remove_punct (boolean): removes punctuation \n",
    "    remove_alpha (boolean): removes non-alphabetic tokens\n",
    "    remove_stopwords (boolean): removes stopwords\n",
    "    add_custom_stopwords (list): adds custom stopwords\n",
    "    lemma (boolean): lemmantises tokens\n",
    "    stem (boolean): stems tokes using the Porter Stemmer\n",
    "    Output: \n",
    "    tokens (list): a list of cleaning tokens\n",
    "    '''\n",
    "    if remove_accents == True:\n",
    "        text = strip_accents(text)\n",
    "    if lower == True:\n",
    "        text = text.lower()\n",
    "    if remove_url == True:\n",
    "            text = re.sub(r'http\\S+', '', text)\n",
    "    #tokens = simple_preprocess(text, deacc=remove_accents, min_len=remove_less_than, max_len=remove_more_than)\n",
    "    tokens = text.split()\n",
    "    if remove_punct == True:\n",
    "        tokens = [ch.translate(str.maketrans('', '', string.punctuation)) for ch in tokens]\n",
    "    if remove_alpha == True:\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "    if remove_stopwords == True:\n",
    "        for i in add_custom_stopwords:\n",
    "            stop_words.add(i)\n",
    "        tokens = [token for token in tokens if not token in stop_words]\n",
    "    tokens = [i for i in tokens if remove_less_than <=  len(i) <= remove_more_than]\n",
    "    if lemma == True: \n",
    "        tokens = [WordNetLemmatizer().lemmatize(token) for token in tokens]\n",
    "    if stem == True:\n",
    "        tokens = [PorterStemmer().stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a word2vec model\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# For word2vec, we need a list tokens\n",
    "scandal_in_bohemia_tokens = [preprocess(i) for i in scandal_in_bohemia_sentences]\n",
    "w2v_model = gensim.models.Word2Vec(scandal_in_bohemia_tokens, size = 500, window = 10, min_count=1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a fasttext model - one with and without subwords\n",
    "# Fasttext takes as an input, a txt file in the environment - so we refer to the raw file and not the one in our Python environment\n",
    "# The input file here is the same as the list of sentences we have already seen but with stop words removed\n",
    "\n",
    "ft_model = fasttext.train_unsupervised('scandal_in_bohemia_sentences_no_stopwords.txt', minn = 2, maxn = 5, dim = 500)\n",
    "ft_model_wo_subwords = fasttext.train_unsupervised('scandal_in_bohemia_sentences_no_stopwords.txt', maxn = 0, dim = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hand', 0.14927232265472412),\n",
       " ('remove', 0.1452692747116089),\n",
       " ('peculiar', 0.1419636309146881),\n",
       " ('intention', 0.14183956384658813),\n",
       " ('godfrey', 0.13988855481147766),\n",
       " ('fro', 0.1376987099647522),\n",
       " ('driver', 0.13480930030345917),\n",
       " ('came', 0.13391801714897156),\n",
       " ('strange', 0.12853017449378967),\n",
       " ('coming', 0.1273207664489746),\n",
       " ('anything', 0.1268802285194397),\n",
       " ('read', 0.1251574605703354),\n",
       " ('black', 0.12454517185688019),\n",
       " ('numerous', 0.1207168847322464),\n",
       " ('annoyance', 0.11609496176242828),\n",
       " ('hot', 0.11296975612640381),\n",
       " ('fortunate', 0.11278606951236725),\n",
       " ('half', 0.11249668151140213),\n",
       " ('waving', 0.10978461802005768),\n",
       " ('country', 0.10923135280609131)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing the outputs from each model\n",
    "w2v_model.wv.most_similar('woman', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3996190130710602, 'german'),\n",
       " (0.37522637844085693, 'gentleman'),\n",
       " (0.36704689264297485, 'clergyman'),\n",
       " (0.3200114369392395, '</s>'),\n",
       " (0.2853983938694, 'hand'),\n",
       " (0.272637277841568, 'lodge'),\n",
       " (0.26899453997612, 'window'),\n",
       " (0.2592756748199463, 'norton'),\n",
       " (0.2512776553630829, 'note'),\n",
       " (0.239231139421463, 'fire'),\n",
       " (0.2387862354516983, 'understand'),\n",
       " (0.2316322773694992, 'corner'),\n",
       " (0.22655518352985382, 'irene'),\n",
       " (0.22640398144721985, 'matter'),\n",
       " (0.2216232419013977, 'found'),\n",
       " (0.22161732614040375, 'street'),\n",
       " (0.21747052669525146, 'moment'),\n",
       " (0.20840883255004883, 'love'),\n",
       " (0.20640386641025543, 'serpentine'),\n",
       " (0.20150649547576904, 'adler')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.get_nearest_neighbors('woman', k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.15502291917800903, 'cried'),\n",
       " (0.08252811431884766, 'baker'),\n",
       " (0.06099469214677811, 'address'),\n",
       " (0.05087444186210632, 'soul'),\n",
       " (0.05002214387059212, 'sitting'),\n",
       " (0.04498187080025673, 'watson'),\n",
       " (0.04359939694404602, 'hand'),\n",
       " (0.03805050253868103, 'dear'),\n",
       " (0.03730574622750282, 'bohemia'),\n",
       " (0.036167293787002563, 'heard'),\n",
       " (0.036113884299993515, 'understand'),\n",
       " (0.034881722182035446, 'client'),\n",
       " (0.034294936805963516, 'hands'),\n",
       " (0.02885563112795353, 'lady'),\n",
       " (0.026660935953259468, 'mask'),\n",
       " (0.026312250643968582, 'visitor'),\n",
       " (0.0246898103505373, 'clock'),\n",
       " (0.024095812812447548, 'clergyman'),\n",
       " (0.022810054942965508, 'german'),\n",
       " (0.01706317439675331, 'matter')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model_wo_subwords.get_nearest_neighbors('woman', k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
